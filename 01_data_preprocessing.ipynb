{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## オープンデータの紹介\n",
    "データ分析の分野で時折登場するNew York City TaxiのTripデータは、\"Registry of Open Data on AWS\"の一つとして登録されています。今回のblogでは、このデータをデータドリフト検出の題材として利用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "NYC taxi Trip Record as a open data  \n",
    "https://registry.opendata.aws/nyc-tlc-trip-records-pds/\n",
    "\n",
    "Example to access the data above  \n",
    "https://github.com/aws-samples/aws-open-data-analytics-notebooks/tree/master/exploring-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting awswrangler\n",
      "  Downloading awswrangler-1.9.0-py3-none-any.whl (133 kB)\n",
      "\u001b[K     |████████████████████████████████| 133 kB 8.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: SQLAlchemy~=1.3.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.3.15)\n",
      "Collecting sqlalchemy-redshift<0.9.0,>=0.7.0\n",
      "  Downloading sqlalchemy_redshift-0.8.1-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: numpy<1.20.0,>=1.18.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.18.1)\n",
      "Collecting pymysql<0.11.0,>=0.9.0\n",
      "  Downloading PyMySQL-0.10.0-py2.py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 7.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting pyarrow~=1.0.0\n",
      "  Downloading pyarrow-1.0.1-cp36-cp36m-manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.3 MB 11.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting psycopg2-binary~=2.8.0\n",
      "  Downloading psycopg2_binary-2.8.5-cp36-cp36m-manylinux1_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 67.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas<1.2.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.0.5)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.15.49 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.17.42)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.12.49 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from awswrangler) (1.14.42)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from sqlalchemy-redshift<0.9.0,>=0.7.0->awswrangler) (20.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas<1.2.0,>=1.0.0->awswrangler) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas<1.2.0,>=1.0.0->awswrangler) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.26,>=1.20; python_version != \"3.4\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<2.0.0,>=1.15.49->awswrangler) (1.25.8)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<2.0.0,>=1.15.49->awswrangler) (0.15.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<2.0.0,>=1.15.49->awswrangler) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3<2.0.0,>=1.12.49->awswrangler) (0.3.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->sqlalchemy-redshift<0.9.0,>=0.7.0->awswrangler) (1.14.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from packaging->sqlalchemy-redshift<0.9.0,>=0.7.0->awswrangler) (2.4.6)\n",
      "Installing collected packages: sqlalchemy-redshift, pymysql, pyarrow, psycopg2-binary, awswrangler\n",
      "Successfully installed awswrangler-1.9.0 psycopg2-binary-2.8.5 pyarrow-1.0.1 pymysql-0.10.0 sqlalchemy-redshift-0.8.1\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_parse_csv(cab_color, year, month):\n",
    "    # Set next month as string\n",
    "    if int(month) < 12:\n",
    "        limit_year = year\n",
    "        limit_month = str(int(month) + 1).zfill(2)\n",
    "    else:\n",
    "        limit_year = str(int(year) + 1).zfill(4)\n",
    "        limit_month = '01'\n",
    "    \n",
    "    df = wr.s3.read_csv(f's3://nyc-tlc/trip data/{cab_color}_tripdata_{year}-{month}.csv')\n",
    "\n",
    "    # Convert pickup/drop timestamp string to timestamp data type\n",
    "    if cab_color == 'yellow':\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['tpep_pickup_datetime'])\n",
    "        df['dropoff_datetime'] = pd.to_datetime(df['tpep_dropoff_datetime'])\n",
    "    elif cab_color == 'green':\n",
    "        df['pickup_datetime'] = pd.to_datetime(df['lpep_pickup_datetime'])\n",
    "        df['dropoff_datetime'] = pd.to_datetime(df['lpep_dropoff_datetime'])\n",
    "    else:\n",
    "        raise Exception('Unexpected cab_color type')\n",
    "\n",
    "    df['trip_duration_sec'] = df.apply(lambda x: (x.dropoff_datetime - x.pickup_datetime).total_seconds(), axis=1)\n",
    "    df['pickup_ymdh'] = df.pickup_datetime.dt.strftime('%Y-%m-%d %H:00:00')\n",
    "    df['date'] = df.pickup_datetime.dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Drop abnormal records based on trip duration (sec) and pickup timestamp\n",
    "    df = df[(df.trip_duration_sec >= 10) & (df.trip_duration_sec <= 14400)]\n",
    "    df = df[(df.pickup_ymdh >= f'{year}-{month}-01 00:00:00') & (df.pickup_ymdh < f'{limit_year}-{limit_month}-01 00:00:00')].copy()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trip_summary(df_records):\n",
    "    groupby_target = ['date', 'pickup_ymdh', 'PULocationID']\n",
    "    #groupby_target = ['date', 'pickup_ymdh']\n",
    "    calc_target = ['trip_duration_sec', 'fare_amount', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount']\n",
    "\n",
    "    g = df_records.groupby(by=groupby_target)\n",
    "    df_count = g.VendorID.count().rename('count')\n",
    "    df_mean = g[calc_target].mean().rename(columns={x:f'{x}_mean' for x in calc_target})\n",
    "    df_std = g[calc_target].std().rename(columns={x:f'{x}_std' for x in calc_target})\n",
    "\n",
    "    df_summary = pd.concat([df_count, df_mean, df_std], axis=1).reset_index()\n",
    "    \n",
    "    return df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_s3_by_date(df_summary, cab_color, bucket, prefix):\n",
    "    ymd_list = sorted(list(set(df_summary.date.tolist())))\n",
    "    for ymd in ymd_list:\n",
    "        file_ymd = ymd.replace('-', '')\n",
    "        df_save = df_summary[df_summary.date == ymd]\n",
    "        wr.s3.to_csv(df_save, f's3://{bucket}/{prefix}/nyctaxi_tripdata_{cab_color}_{file_ymd}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tripデータの取得と前処理\n",
    "NYC Taxiのtripデータは、タクシーの乗車ごとの時刻、乗車区域、降車区域、料金などの情報で成り立っています。今回は全体としての傾向をつかむために、2019年1月から2020年6月までのデータを取得した上で、1時間ごとの乗車回数や平均の乗車時間、料金などにサマリーしてから自分自身のS3 Bucketに保存します。次のステップで、サマリーしたデータを元にデータドリフトの発生有無を見ていきましょう"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NYC Taxiにはマンハッタン島で乗客を乗せられるYellowライセンスと、マンハッタン島以外で乗客を乗せられるGreenライセンスがあり、それぞれのデータは別のファイルとして提供されています。Yellowライセンスの乗車データは1ヶ月あたり600-700MBと大きくて処理に時間がかかるので、今回はデータ量が1/10程度に収まるGreenライセンスの乗車データを対象にします。マンハッタン島の乗車データを分析してみたい方は、\"cob_color\"を\"yellow\"に設定してチャレンジしてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read NYC cab trip open data, generate summary data and save it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:10: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n"
     ]
    }
   ],
   "source": [
    "year_month = [('2019', '01'), ('2019', '02'), ('2019', '03'), ('2019', '04'), \n",
    "              ('2019', '05'), ('2019', '06'), ('2019', '07'), ('2019', '08'), \n",
    "              ('2019', '09'), ('2019', '10'), ('2019', '11'), ('2019', '12'),\n",
    "              ('2020', '01'), ('2020', '02'), ('2020', '03'), ('2020', '04'), \n",
    "              ('2020', '05'), ('2020', '06')]\n",
    "cab_color = 'green'\n",
    "bucket = 'tuki-bkt-misc'\n",
    "prefix = 'data/nyctaxi/daily_w_location'\n",
    "\n",
    "for year, month in year_month:\n",
    "    df_records = read_and_parse_csv(cab_color, year, month)\n",
    "    df_summary = generate_trip_summary(df_records)\n",
    "    save_s3_by_date(df_summary, cab_color, bucket, prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_records.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_summary.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reading CSV\n",
    "wr.s3.read_csv('s3://tuki-bkt-misc/data/nyctaxi/summary_daily/nyctaxi_tripdata_green_20200715.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
